{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "\n",
    "cases and deaths by country and state.\n",
    "\n",
    "Datasets wanted:\n",
    "\n",
    "- location data\n",
    "- testing by country and state\n",
    "- population age structure? \n",
    "- lockdowns (and other interventions) by country and state\n",
    "\n",
    "Plot ideas:\n",
    "\n",
    "- plot rank change to see which ones are trending worse and which are trending better\n",
    "- plot dpm change / dpm to plot the growth rate (slope) over time\n",
    "- plt dpm change to plot the rate * base - base, which relates to hospital capacity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from scipy.special import gamma\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cyprus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('data')\n",
    "\n",
    "# https://www.gov.uk/eu-eea\n",
    "eu_countries = ['Austria', 'Belgium', 'Bulgaria', 'Croatia', 'Cyprus', 'Czech Republic', \n",
    "                'Denmark', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Ireland',\n",
    "                'Italy', 'Latvia', 'Lithuania', 'Luxembourg', 'Malta', 'Netherlands', 'Poland', \n",
    "                'Portugal', 'Romania', 'Slovakia', 'Slovenia', 'Spain', 'Sweden']\n",
    "\n",
    "\n",
    "def load_un_country_population_data():\n",
    "    # Adding country population data...\n",
    "    # map UN country names to OWID country names\n",
    "    u2o = dict([\n",
    "         ('Bolivia (Plurinational State of)', 'Bolivia'),\n",
    "         ('Brunei Darussalam', 'Brunei'),\n",
    "         ('Cabo Verde', 'Cape Verde'),\n",
    "         (\"Côte d'Ivoire\", \"Cote d'Ivoire\"),\n",
    "         ('Curaçao', 'Curacao'),\n",
    "         ('Czechia', 'Czech Republic'),\n",
    "         ('Democratic Republic of the Congo', 'Democratic Republic of Congo'),\n",
    "         ('Faroe Islands', 'Faeroe Islands'),\n",
    "         ('Falkland Islands (Malvinas)', 'Falkland Islands'),\n",
    "         ('Iran (Islamic Republic of)', 'Iran'),\n",
    "         (\"Lao People's Democratic Republic\", 'Laos'),\n",
    "         ('North Macedonia', 'Macedonia'),\n",
    "         ('Northern Mariana Islands', 'Mariana Islands'),\n",
    "         ('Republic of Moldova', 'Moldova'),\n",
    "         ('State of Palestine', 'Palestine'),\n",
    "         ('Russian Federation', 'Russia'),\n",
    "         ('Republic of Korea', 'South Korea'),\n",
    "         ('Eswatini', 'Swaziland'),\n",
    "         ('Syrian Arab Republic', 'Syria'),\n",
    "         ('China, Taiwan Province of China', 'Taiwan'),\n",
    "         ('United Republic of Tanzania', 'Tanzania'),\n",
    "         ('Timor-Leste', 'Timor'),\n",
    "         ('United States of America', 'United States'),\n",
    "         ('Holy See', 'Vatican'),\n",
    "         ('Venezuela (Bolivarian Republic of)', 'Venezuela'),\n",
    "         ('Viet Nam', 'Vietnam')\n",
    "    ])\n",
    "    # Load UN population data\n",
    "    pop_df = pd.read_csv(DATA_DIR / 'WPP2019_TotalPopulationBySex.csv')\n",
    "    # Use the medium variant projection for the year 2020 of total population\n",
    "    pop_df = pop_df.loc[(pop_df['Time'] == 2020) & (pop_df['Variant'] == 'Medium'), ['Location', 'PopTotal']]\n",
    "    # Convert Locations to match OWID country names\n",
    "    pop_df['Location'] = pop_df['Location'].apply(lambda l: l if l not in u2o else u2o[l])\n",
    "    pop_df['PopTotal'] = pop_df['PopTotal'] * 1000\n",
    "    pop_df = pop_df.rename(columns={'PopTotal': 'population'})\n",
    "    return pop_df\n",
    "\n",
    "\n",
    "def load_owid_country_data(download=False, cache=False):\n",
    "    # country covid data\n",
    "    url = 'https://covid.ourworldindata.org/data/ecdc/full_data.csv'\n",
    "    filename = DATA_DIR / 'full_data.csv' # cached version\n",
    "    if download:\n",
    "        df = pd.read_csv(url)\n",
    "        if cache:\n",
    "            df.to_csv(filename)\n",
    "    else:\n",
    "        df = pd.read_csv(filename)\n",
    "    \n",
    "    df = df.rename(columns={'total_cases': 'cases', \n",
    "                                          'total_deaths': 'deaths',\n",
    "                                          'location': 'entity'})\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    pop_df = load_un_country_population_data()\n",
    "    df = (df.merge(pop_df, how='inner', left_on='entity', right_on='Location')\n",
    "          .drop(columns='Location').rename(columns={'PopTotal': 'population'}))\n",
    "    \n",
    "    ## Add Eurpean Union as an entity\n",
    "    # sanity check: all EU countries are in country population data\n",
    "    assert len(eu_countries) == len(pop_df.loc[pop_df['Location'].isin(eu_countries), 'Location'])\n",
    "    eu_pop = pop_df.loc[pop_df['Location'].isin(eu_countries), 'population'].sum()\n",
    "    # aggregate deaths, cases, new_cases, new_deaths for eu countries\n",
    "    eu_df = (df.loc[df['entity'].isin(eu_countries)].groupby('date').aggregate(np.sum)\n",
    "             .reset_index().assign(entity='EU'))\n",
    "    eu_df['population'] = eu_pop\n",
    "    df = df.append(eu_df, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_nytimes_us_state_data(download=False, cache=False):\n",
    "    '''\n",
    "    nytimes dataset with cases and deaths (cummulative) for US states.\n",
    "    '''\n",
    "    url = 'https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv'\n",
    "    filename = DATA_DIR / 'us-states.csv'\n",
    "    if download:\n",
    "        df = pd.read_csv(url)\n",
    "        if cache:\n",
    "            df.to_csv(filename)\n",
    "    else:\n",
    "        df = pd.read_csv(filename)\n",
    "    \n",
    "    df = df.rename(columns={'state': 'entity'})\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Add state population, used to get deaths per million\n",
    "    pop_df = (pd.read_csv(DATA_DIR / 'nst-est2019-alldata.csv')\n",
    "              .loc[lambda d: d['SUMLEV'] == 40, ]\n",
    "             )\n",
    "    df = (df.merge(pop_df, how='inner', left_on='entity', right_on='NAME')\n",
    "           .loc[:, ['date', 'entity', 'cases', 'deaths', 'POPESTIMATE2019']]\n",
    "           .rename(columns={'POPESTIMATE2019': 'population'})\n",
    "          )\n",
    "    \n",
    "    # Add daily change\n",
    "    grouped = df.groupby(['entity'])\n",
    "    df['new_cases'] = grouped['cases'].transform(\n",
    "        lambda s: s.rolling(2).apply(lambda w: w.iloc[1] - w.iloc[0]))\n",
    "    df['new_deaths'] = grouped['deaths'].transform(\n",
    "        lambda s: s.rolling(2).apply(lambda w: w.iloc[1] - w.iloc[0]))\n",
    "    df['cases_daily_growth_rate'] = grouped['cases'].transform(\n",
    "        lambda s: s.rolling(2).apply(lambda w: w.iloc[1] / w.iloc[0]))\n",
    "    df['deaths_daily_growth_rate'] = grouped['deaths'].transform(\n",
    "        lambda s: s.rolling(2).apply(lambda w: w.iloc[1] / w.iloc[0]))\n",
    "    \n",
    "    return df\n",
    "   \n",
    "\n",
    "def make_nday_ratio(df, col, n=1):\n",
    "    return df.groupby(['entity'])[col].transform(\n",
    "        lambda s: s.rolling(2).apply(lambda w: w.iloc[-1] / w.iloc[0]))\n",
    "    \n",
    "    \n",
    "def make_nday_diff(df, col, n=1):\n",
    "    return df.groupby(['entity'])[col].transform(\n",
    "        lambda s: s.rolling(2).apply(lambda w: w.iloc[-1] - w.iloc[0]))\n",
    "    \n",
    "\n",
    "def before_threshold(df, col, thresh):\n",
    "    '''\n",
    "    Return boolean index of df rows before the first day when the threshold\n",
    "    was met, i.e. when df[col] >= thresh. This is done with each entity.\n",
    "    :param df: entity/state/country column, ordered by date column, cases, deaths, etc.\n",
    "    '''\n",
    "    return df.groupby(['entity'])[col].transform(lambda s: (s >= thresh).cumsum() == 0)\n",
    "\n",
    "\n",
    "def fill_before_first(df, col, thresh, thresh_col=None, fill=np.nan):\n",
    "    '''\n",
    "    Within each entity, df[col] = fill when df[thresh_col] < thresh \n",
    "    and occur before the first day that df[thresh_col] >= threshold.\n",
    "    :param col: the column to fill\n",
    "    :param thresh_col: if not None, use this column to determine which rows\n",
    "    '''\n",
    "    return df[col].where(~before_threshold(df, thresh_col, thresh),\n",
    "                         np.nan)\n",
    "\n",
    "\n",
    "def make_days_since(df, col, thresh):\n",
    "    '''\n",
    "    Days since first day the value of col >= thresh. \n",
    "    Example of column and threshold from data journalists:\n",
    "    - days since deaths per million reached 0.1 deaths per million\n",
    "      (OWID deaths per million trajectories).\n",
    "    - days since 3 daily deaths first recorded (FT daily deaths trajectory)\n",
    "      https://twitter.com/jburnmurdoch/status/1245466020053164034\n",
    "      \n",
    "    Example usage:\n",
    "        df['days_since'] = make_days_since(df, 'deaths_per_million', 0.1)\n",
    "    '''\n",
    "    # >>> a = np.array([[1,1,1,1],[1,2,1,2],[1,1,2,2],[2,2,2,2]]).astype(float)\n",
    "    # >>> a\n",
    "    # array([[1., 1., 1., 1.],\n",
    "    #        [1., 2., 1., 2.],\n",
    "    #        [1., 1., 2., 2.],\n",
    "    #        [2., 2., 2., 2.]])\n",
    "    # >>> b = ((a > 1).cumsum(axis=1) > 0).cumsum(axis=1).astype(float) - 1\n",
    "    # >>> b[b < 0] = np.nan\n",
    "    # >>> b\n",
    "    # array([[nan, nan, nan, nan],\n",
    "    #        [nan,  0.,  1.,  2.],\n",
    "    #        [nan, nan,  0.,  1.],\n",
    "    #        [ 0.,  1.,  2.,  3.]])\n",
    "    def days_since(s, thresh):\n",
    "        '''s: a Series containing the values of col for an entity ordered by date'''\n",
    "        days = ((s >= thresh).cumsum() > 0).cumsum().astype(float) - 1\n",
    "        days[days < 0] = np.nan\n",
    "        return days\n",
    "        \n",
    "    return df.groupby(['entity'])[col].transform(days_since, thresh=thresh)\n",
    "\n",
    "\n",
    "def add_derived_values_cols(df, values):\n",
    "    '''\n",
    "    deaths_per_million\n",
    "    deaths_per_day\n",
    "    deaths_per_day_7day_avg\n",
    "    deaths_per_week\n",
    "    deaths_per_million_per_day\n",
    "    deaths_per_million_per_day_7day_avg\n",
    "    deaths_per_million_per_week\n",
    "    deaths_ratio_per_day\n",
    "    deaths_ratio_per_day_7day_avg\n",
    "    deaths_ratio_per_week\n",
    "    '''\n",
    "    values_per_million = values + '_per_million'\n",
    "    df[values_per_million] = df[values] / df['population'] * 1e6\n",
    "\n",
    "    values_per_day = values + '_per_day'\n",
    "    df[values_per_day] = df.groupby(['entity'])[values].transform(\n",
    "        lambda s: s.rolling(2).apply(lambda w: w.iloc[-1] - w.iloc[0]))\n",
    "\n",
    "    # FT: 7-day rolling average of daily change in deaths\n",
    "    # https://www.ft.com/coronavirus-latest\n",
    "    values_per_day_7day_avg = values_per_day + '_7day_avg'\n",
    "    df[values_per_day_7day_avg] = (df.groupby(['entity'])[values_per_day]\n",
    "                                               .transform(lambda s: s.rolling(7).mean()))                                   \n",
    "\n",
    "    values_per_week = values + '_per_week'\n",
    "    df[values_per_week] = df.groupby(['entity'])[values].transform(\n",
    "        lambda s: s.rolling(8).apply(lambda w: w.iloc[-1] - w.iloc[0]))\n",
    "\n",
    "    values_per_million_per_day = values_per_million + '_per_day'\n",
    "    df[values_per_million_per_day] = df.groupby(['entity'])[values_per_million].transform(\n",
    "        lambda s: s.rolling(2).apply(lambda w: w.iloc[-1] - w.iloc[0]))\n",
    "\n",
    "    # Per capita alternative to unnormalized FT values\n",
    "    values_per_million_per_day_7day_avg = values_per_million_per_day + '_7day_avg'\n",
    "    df[values_per_million_per_day_7day_avg] = (df.groupby(['entity'])[values_per_million_per_day]\n",
    "                                               .transform(lambda s: s.rolling(7).mean()))                                   \n",
    "\n",
    "    values_per_million_per_week = values_per_million + '_per_week'\n",
    "    df[values_per_million_per_week] = df.groupby(['entity'])[values_per_million].transform(\n",
    "        lambda s: s.rolling(8).apply(lambda w: w.iloc[-1] - w.iloc[0]))\n",
    "\n",
    "    values_ratio_per_day = values + '_ratio_per_day'\n",
    "    df[values_ratio_per_day] = df.groupby(['entity'])[values].transform(\n",
    "        lambda s: s.rolling(2).apply(lambda w: w.iloc[-1] / w.iloc[0]))\n",
    "\n",
    "    values_ratio_per_day_7day_avg = values_ratio_per_day + '_7day_avg'\n",
    "    df[values_ratio_per_day_7day_avg] = (df.groupby(['entity'])[values_ratio_per_day]\n",
    "                                               .transform(lambda s: s.rolling(7).mean()))                                   \n",
    "    \n",
    "    values_ratio_per_week = values + '_ratio_per_week'\n",
    "    df[values_ratio_per_week] = df.groupby(['entity'])[values].transform(\n",
    "        lambda s: s.rolling(8).apply(lambda w: w.iloc[-1] / w.iloc[0]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# load data\n",
    "# df['days_since'] = make_days_since(df, 'deaths_per_million', 0.1)\n",
    "def load_all(download=False):\n",
    "    download = False\n",
    "    states_df = (load_nytimes_us_state_data(download=download, cache=True)\n",
    "                 .pipe(add_derived_values_cols, 'deaths')\n",
    "                 .pipe(add_derived_values_cols, 'cases'))\n",
    "    display(states_df.iloc[-1:])\n",
    "    print(f'states_df.shape: {states_df.shape}')\n",
    "    countries_df = (load_owid_country_data(download=download, cache=True)\n",
    "                    .pipe(add_derived_values_cols, 'deaths')\n",
    "                    .pipe(add_derived_values_cols, 'cases'))\n",
    "    display(countries_df.iloc[-1:])\n",
    "    print(f'countries_df.shape: {countries_df.shape}')\n",
    "    # combine countries and states, dropping the countries 'Puerto Rico' (b/c it is in states_df) and 'Georgia' (b/c a state is named 'Georgia')\n",
    "    all_df = (countries_df.loc[~countries_df['entity'].isin(['Georgia', 'Puerto Rico']), ['date', 'entity', 'cases', 'deaths', 'population']]\n",
    "              .append(states_df.loc[:, ['date', 'entity', 'cases', 'deaths', 'population']], ignore_index=True)\n",
    "              .reset_index(drop=True)\n",
    "              .pipe(add_derived_values_cols, 'deaths')\n",
    "              .pipe(add_derived_values_cols, 'cases'))              \n",
    "    display(all_df.iloc[-1:])\n",
    "    print(f'all_df.shape: {all_df.shape}')\n",
    "    return states_df, countries_df, all_df\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure out how to map UN Locations to OWID Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_owid_to_un_pop(mer=None):\n",
    "    '''\n",
    "    Print where owid countries and un countries do not line up.\n",
    "    Visually inspect to build a mapping so that all OWID countries map.\n",
    "    '''\n",
    "    # convert UN country names to OWID country names.\n",
    "    # 'Guernsey': None,  # No population info in UN csv.\n",
    "    # 'International': None,  # Not a country. :-)\n",
    "    # 'Jersey': None, # Guernsey and Jersey are british channel islands\n",
    "    # 'Kosovo': None, # Republic of Kosovo\n",
    "    u2o = dict([\n",
    "         ('Bolivia (Plurinational State of)', 'Bolivia'),\n",
    "         ('Brunei Darussalam', 'Brunei'),\n",
    "         ('Cabo Verde', 'Cape Verde'),\n",
    "         (\"Côte d'Ivoire\", \"Cote d'Ivoire\"),\n",
    "         ('Curaçao', 'Curacao'),\n",
    "         ('Czechia', 'Czech Republic'),\n",
    "         ('Democratic Republic of the Congo', 'Democratic Republic of Congo'),\n",
    "         ('Faroe Islands', 'Faeroe Islands'),\n",
    "         ('Falkland Islands (Malvinas)', 'Falkland Islands'),\n",
    "         ('Iran (Islamic Republic of)', 'Iran'),\n",
    "         (\"Lao People's Democratic Republic\", 'Laos'),\n",
    "         ('North Macedonia', 'Macedonia'),\n",
    "         ('Northern Mariana Islands', 'Mariana Islands'),\n",
    "         ('Republic of Moldova', 'Moldova'),\n",
    "         ('State of Palestine', 'Palestine'),\n",
    "         ('Russian Federation', 'Russia'),\n",
    "         ('Republic of Korea', 'South Korea'),\n",
    "         ('Eswatini', 'Swaziland'),\n",
    "         ('Syrian Arab Republic', 'Syria'),\n",
    "         ('China, Taiwan Province of China', 'Taiwan'),\n",
    "         ('United Republic of Tanzania', 'Tanzania'),\n",
    "         ('Timor-Leste', 'Timor'),\n",
    "         ('United States of America', 'United States'),\n",
    "         ('Holy See', 'Vatican'),\n",
    "         ('Venezuela (Bolivarian Republic of)', 'Venezuela'),\n",
    "         ('Viet Nam', 'Vietnam')\n",
    "    ])\n",
    "\n",
    "    if mer is None:\n",
    "        pop_df = pd.read_csv(DATA_DIR / 'WPP2019_TotalPopulationBySex.csv')\n",
    "        # Convert Locations to match OWID country names\n",
    "        pop_df['Location'] = pop_df['Location'].apply(lambda l: l if l not in u2o else u2o[l])\n",
    "        # Use the medium variant projection for the year 2020 of total population\n",
    "        pop_df = pop_df.loc[(pop_df['Time'] == 2020) & (pop_df['Variant'] == 'Medium'), ['Location', 'PopTotal']]\n",
    "        df = load_owid_country_data(download=True, cache=False)\n",
    "        mer = df.merge(pop_df, how='outer', left_on='entity', right_on='Location')\n",
    "    \n",
    "    \n",
    "    print('OWID entity not found in UN pop Location')\n",
    "    print(mer.loc[mer['Location'].isna(), 'entity'].unique())\n",
    "    print('UN pop Location not found in OWID entity')\n",
    "    print(mer.loc[mer['entity'].isna(), 'Location'].unique())\n",
    "    return mer\n",
    "\n",
    "# mer = map_owid_to_un_pop()\n",
    "# mer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a minute\n",
    "states_df, countries_df, all_df = load_all(download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'What states have a population similar to New Zealand?')\n",
    "print(list(states_df.loc[(states_df['population'] < 5e6) & (states_df['population'] > 3e6), :]\n",
    "           .groupby('entity').first().sort_values(by='population').index))\n",
    "print(f'What overlap is there between countries and states?')\n",
    "print(set(states_df['entity']).intersection(countries_df['entity']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Vermont Coronavirus Cases vs Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = countries_df\n",
    "# entity = 'New Zealand'\n",
    "df = states_df\n",
    "entity = 'Vermont'\n",
    "df = df.loc[df['entity'] == entity, :]\n",
    "for var in ['cases', 'deaths']:\n",
    "    plt.plot(df['date'], df[var], label=var)\n",
    "#     plt.semilogy(df['date'], df[var], label=var)\n",
    "    \n",
    "plt.xticks(rotation=-45)\n",
    "plt.ylabel('total')\n",
    "plt.xlabel('date')\n",
    "plt.title(f'Covid-19 in {entity}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "df.loc[df['date'] > pd.to_datetime('2020-04-01')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory Plots for deaths or deaths per million vs Date or Days Since\n",
    "\n",
    "An excellent example of a similar plot of log(DPM) vs days since 0.1 DPM is https://ourworldindata.org/grapher/covid-deaths-days-since-per-million. It visually shows which countries are on similar trajectories, but makes it hard to compare the timeline of when interventions in countries are happening and what their outbreak looked like when those interventions were put in place.\n",
    "\n",
    "Plotting log(DPM) vs date makes it harder to see the similarities of trajectories (though many of the trajectories look similar. Harder to plot those awesome sloped lines that say \"doubles every 2 days\", \"doubles every 3 days\", \"doubles every 5 days\", etc. Maybe an inset protractor-like graphic showing the doubling times.\n",
    "\n",
    "\n",
    "### Hospital Beds\n",
    "\n",
    "What is the medical capacity required to handle the peak of the infection. It depends on how full the beds are, at what rate new patients are coming in, and what rate old patients are leaving (death or recovery).\n",
    "\n",
    "When plotted on a log(deaths per million) scale, what \"slope\" can a given amount of medical capacity (ICU beds, ventilators, healthcare workers, PPE, etc.) handle? Assume that the daily change in deaths per million is proportional (accounting for lag and growth changes) to incoming icu cases. For a steady state between incoming ICU cases and outgoing ICU cases (death and recovery), outgoing ICU cases (deaths and recoveries) must equal incoming ICU cases and therefore be proportional to delta deaths per million. And delta deaths per million relates deaths per million * slope. \n",
    "\n",
    "Why should we care? As deaths per million goes up, the slope must decrease to match hospital capacity. That means the government response must be stronger and more burdensome.\n",
    " \n",
    "If delta_dpm exceeds this amount, than the number of incoming patients exceeds outgoing and must be absorbed by  available medical capacity. If delta_dpm is less than this amount, the number of incoming patients is less than the number of outgoing patients, and available hospital capacity increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal Flu Death Source\n",
    "# https://www.cdc.gov/flu/about/burden/index.html\n",
    "# seasons: 2010-2011, 2011-2012, 2012-2013, 2013-2014, 2014-2015, 2015-2016, 2016-2017, 2017-2018\n",
    "seasonal_flu_deaths = np.array([37000, 12000, 43000, 38000, 51000, 23000, 38000, 61000])\n",
    "# US Population Source\n",
    "# https://www.worldometers.info/world-population/us-population/\n",
    "us_pop = 331000000 # estimated 2020 population\n",
    "seasonal_flu_dpm = seasonal_flu_deaths.mean() / us_pop * 1e6\n",
    "bad_flu_dpm = seasonal_flu_deaths.max() / us_pop * 1e6\n",
    "print(f'seasonal_flu_dpm: {seasonal_flu_dpm}, bad_flu_dpm: {bad_flu_dpm}')\n",
    "\n",
    "\n",
    "def plot_trajectories(df, index_col='date', values_col='deaths', rank=False, n_top=None,\n",
    "                      includes=None, excludes=None, n_show=None):\n",
    "    '''\n",
    "    df: columns: date, deaths, cases, population\n",
    "    index_col: either date or days_since. This is the pivot index.\n",
    "    values_col: e.g. deaths, deaths_per_million, deaths_per_day, deaths_per_million_per_day\n",
    "    rank: plot the ranks of the values within each day.\n",
    "    n_top: int > 0. Display the n highest trajectories on the chart.\n",
    "    n_show: int > 0. Show at most n on the chart.\n",
    "    includes: array of entities to highlight.\n",
    "    excludes: array of entities to exclude from chart.\n",
    "    '''\n",
    "    \n",
    "    # https://stackoverflow.com/questions/13851535/delete-rows-from-a-pandas-dataframe-based-on-a-conditional-expression-involving\n",
    "    if excludes:\n",
    "        df = df.loc[~df['entity'].isin(excludes), :]\n",
    "        \n",
    "    # remove obsevations without values\n",
    "    df = df.loc[df[values_col].notna(), :] \n",
    "    # remove observations without index value \n",
    "    # for example, days that happen before the first day of the days_since column\n",
    "    df = df.loc[df[index_col].notna(), :]\n",
    "\n",
    "    # Pivot to a table with country/entity columns and date/days_since rows\n",
    "    piv = df.pivot(index=index_col, columns='entity', values=values_col)\n",
    "    piv = piv.loc[piv.notnull().any(axis=1), :]  # remove rows with all null values\n",
    "\n",
    "    # entities ranked by each day, or by each day since 0.1.\n",
    "    if rank:\n",
    "        piv = piv.rank(axis=1, method='average', ascending=False)\n",
    "  \n",
    "    # Plot countries in order, sorting by the most recent value for each entity.\n",
    "    # For days_since, the last value can be nan. Find the most recent non-nan value.\n",
    "    sort_idx = np.argsort(piv.apply(lambda s: s[s.notna()].iloc[-1], axis=0))\n",
    "    if not rank:\n",
    "        # plot deaths per million from largest to smallest\n",
    "        sort_idx = sort_idx[::-1]\n",
    "    \n",
    "    # Choose which entitites to plot\n",
    "    sorted_entities = piv.columns.values[sort_idx]\n",
    "#     print('Num sorted_entities:', len(sorted_entities))\n",
    "    n_ent = len(sorted_entities) \n",
    "    n_top = n_ent if n_top is None else n_top\n",
    "    n_show = n_ent if n_show is None else n_show\n",
    "    includes_idx = np.isin(sorted_entities, includes) if includes else np.zeros_like(sorted_entities, dtype=bool)\n",
    "    excludes_idx = np.isin(sorted_entities, excludes) if excludes else np.zeros_like(sorted_entities, dtype=bool)\n",
    "    priority_idx = np.hstack([np.arange(n_ent)[includes_idx & ~excludes_idx],\n",
    "                              np.arange(n_ent)[~includes_idx & ~excludes_idx]])\n",
    "    show_entities = sorted_entities[np.sort(priority_idx[:n_show])]\n",
    "    top_entities = sorted_entities[np.sort(priority_idx[:n_top])]\n",
    "    print(len(show_entities), show_entities)\n",
    "    # Figure\n",
    "    fig, ax = plt.subplots(figsize=(16,8))\n",
    "    for i, entity in enumerate(show_entities):\n",
    "        if entity in top_entities:\n",
    "            linewidth = 2.0\n",
    "            alpha = 1.0\n",
    "            entity_rank = np.arange(n_ent)[sorted_entities == entity][0] + 1\n",
    "            label = f'{entity}[{entity_rank}]'\n",
    "            annotation = entity\n",
    "            last_idx = piv.index[piv[entity].notna()].values[-1]\n",
    "        else:\n",
    "            linewidth = 1.0\n",
    "            alpha = 0.5\n",
    "            label = None\n",
    "            annotation = None\n",
    "            \n",
    "        if rank or 'ratio' in values_col:\n",
    "            plt.plot(piv.index, piv[entity], label=label, linewidth=linewidth, alpha=alpha)\n",
    "        else:\n",
    "            plt.semilogy(piv.index, piv[entity], label=label, linewidth=linewidth, alpha=alpha,\n",
    "                         marker='o', markersize='4')\n",
    "            if annotation:\n",
    "                plt.annotate(entity, xy=(last_idx, piv[entity].loc[last_idx]))\n",
    "            \n",
    "\n",
    "    # pivot == 'days_since' maybe does not play well with bad_flu in the data.\n",
    "    if values_col == 'deaths_per_million' and not rank:\n",
    "        plt.axhline(seasonal_flu_dpm, color='blue', linestyle='--', label='seasonal flu')\n",
    "        plt.axhline(bad_flu_dpm, color='orange', linestyle='--', label='bad flu')\n",
    "        \n",
    "    plt.grid(True, which='major')  # add gridlines (major and minor ticks)\n",
    "    \n",
    "    if rank:\n",
    "        ax.invert_yaxis()\n",
    "        \n",
    "    ylabel = values_col\n",
    "    if rank:\n",
    "        ylabel += ' (rank)'\n",
    "    else:\n",
    "        ylabel += ' (log scale)'\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    if index_col == 'date':\n",
    "        plt.xlabel('Date')\n",
    "    elif index_col == 'days_since':\n",
    "        plt.xlabel(f'Days since outbreak began')\n",
    "      \n",
    "    title = f'{values_col}{\"\" if not rank else \" (rank)\"} in Each Location' \n",
    "    title += ' Over Time' if index_col != 'days_since' else f' Since Outbreak Began'\n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.xticks(rotation=-60)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return piv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lockdowns:\n",
    "# Minnesota - stay-at-home 3/27 https://www.mprnews.org/story/2020/03/25/latest-on-covid19-in-minnesota\n",
    "# New Zealand - lockdown 3/25 https://www.washingtonpost.com/world/asia_pacific/new-zealand-isnt-just-flattening-the-curve-its-squashing-it/2020/04/07/6cab3a4a-7822-11ea-a311-adb1344719a9_story.html\n",
    "\n",
    "# df = states_df\n",
    "# df = countries_df\n",
    "df = all_df\n",
    "df['deaths_per_case'] = df['deaths'] / df['cases']\n",
    "\n",
    "# index_col, thresh, values_col = ('days_since', 0.1, 'deaths_per_million')\n",
    "# index_col, thresh, values_col = ('days_since', 1.0, 'deaths_per_million')\n",
    "# index_col, thresh, values_col = ('days_since', 3, 'deaths_per_day')\n",
    "# index_col, thresh, values_col = ('days_since', 20, 'deaths_per_week')\n",
    "# index_col, thresh, values_col = ('date', 0.1, 'deaths_per_million')\n",
    "# index_col, thresh, values_col = ('date', 1.1, 'deaths_ratio_per_week')\n",
    "# index_col, thresh, values_col = ('days_since', 1.1, 'deaths_ratio_per_week')\n",
    "\n",
    "index_col = 'days_since'\n",
    "# index_col = 'date'\n",
    "\n",
    "# values_col = 'deaths_ratio_per_day_7day_avg'\n",
    "values_col = 'deaths_per_million'\n",
    "# values_col = 'deaths_per_day_7day_avg'\n",
    "# values_col = 'deaths_per_million_per_day_7day_avg'\n",
    "# values_col = 'cases_per_million'\n",
    "# values_col = 'cases_per_million_per_day_7day_avg'\n",
    "# values_col = 'deaths_per_case'\n",
    "\n",
    "n_top = 15\n",
    "n_show = 53\n",
    "rank = False\n",
    "# rank = True\n",
    "\n",
    "# FT threshold condition https://www.ft.com/coronavirus-latest 2020-03-02\n",
    "# days_since_col = 'deaths_per_day'; days_since_thresh = 3.0\n",
    "# FT thesh: number of days since 30 daily cases first recorded\n",
    "# days_since_col = 'cases_per_day'; days_since_thresh = 30\n",
    "# OWID threshold condition https://ourworldindata.org/grapher/covid-deaths-days-since-per-million\n",
    "days_since_col = 'deaths_per_million'; days_since_thresh = 0.1\n",
    "# days_since_col = 'cases_per_million'; days_since_thresh = 1\n",
    "\n",
    "# Trim rows before the timeperiod we are looking at:\n",
    "if days_since_col:\n",
    "    df[values_col] = fill_before_first(df, values_col, days_since_thresh, thresh_col=days_since_col)\n",
    "    if index_col == 'days_since':\n",
    "        df['days_since'] = make_days_since(df, days_since_col, days_since_thresh)\n",
    "\n",
    "us_states = list(states_df['entity'].unique())\n",
    "includes = list(set([\n",
    "            'United States',\n",
    "#     'Italy', 'Spain', 'France',\n",
    "#             'Vermont', 'California', 'Washington', 'New York', 'Michigan', 'Louisiana', 'Massachusetts',\n",
    "#             'China',\n",
    "            'New Zealand',\n",
    "    'New York',\n",
    "    'EU',\n",
    "#     countries from https://www.washingtonpost.com/opinions/2020/04/08/is-swedens-lax-approach-coronavirus-backfiring/\n",
    "#     'Sweden', 'Denmark', 'Norway', 'Malta', 'United Kingdom', 'Italy', 'United States',\n",
    "# States ~3-5M in population\n",
    "# 'Arkansas', 'Nevada', 'Iowa', 'Puerto Rico', 'Utah', 'Connecticut', 'Oklahoma', 'Oregon',\n",
    "#              'Kentucky', 'Louisiana', 'Alabama',\n",
    "    \n",
    "]\n",
    "# + us_states\n",
    "# + eu_countries\n",
    "))\n",
    "low_pop_entities = list(df.loc[df['population'] < 1e6, 'entity'].unique())\n",
    "print(f'low population entities\\n{low_pop_entities}')\n",
    "excludes = list(set([\n",
    "# Excludes low-population entities with high deaths-per-capita.\n",
    "'World',    \n",
    "] + low_pop_entities))\n",
    "\n",
    "piv = plot_trajectories(df, index_col=index_col, values_col=values_col, \n",
    "                        rank=rank, n_top=n_top, n_show=n_show,\n",
    "                        includes=includes, excludes=excludes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['entity'] == 'EU']\n",
    "# df['entity'].unique()\n",
    "# countries_df.loc[countries_df['entity'].isin(excludes), ['entity', 'population']].groupby('entity').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['entity']=='New Zealand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
